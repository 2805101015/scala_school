---
prev: finagle.textile
title: Searchbird
layout: post
---

We're going to build a simple distributed search engine using Scala and the previously discussed "Finagle":http://github.com/twitter/finagle framework.
我们要使用Scala和先前讨论的"Finagle":http://github.com/twitter/finagle 框架构建一个简单的分布式搜索引擎。

H3。

h3. Design goals: the big picture 设计目标：大图景

Broadly, our design goals include _abstraction_ (the ability to use the resulting system without knowing all of its internal detail); _modularity_ (the ability to factor the system into smaller, simpler pieces that can be more easily understood and/or replaced with other pieces); and _scalability_ (the ability to grow the capacity of the system in a straightforward way).
从广义上讲，我们的设计目标包括_abstraction抽象_ （有能力在不知道其内部的所有细节的情况下利用由此产生的系统）； _modularity模块化_ （把系统分为较小，较简单的片段，从而更容易被理解和/或被更换的分解能力） ; _scalability扩展性_ （以简单的方式按系统的容量成长的能力）。

我们要描述的系统有三个部分： （1） _clients客户端_发出请求（2） _servers服务器_ ，发送请求的应答，和（3） _transport传送_机制来这些通信包起来。通常情况下，客户端和服务器位于不同的机器上，通过网络上的一个特定的“ _port端口_ ”"_port_":http://en.wikipedia.org/wiki/Port_(computer_networking)进行通信，但是在这个例子中，它们将运行在在同一台机器（而且仍然使用端口进行通信） 。在我们的例子中，客户端和服务器将用Scala编写，运输将使用"Thrift":http://thrift.apache.org/处理。本教程的主要目的是展示一个简单的服务器和客户端，并具有良好的可扩展性。

The system we're going to describe has three pieces: (1) _clients_ that makes requests to (2) _servers_, which send responses to the request; and a (3) _transport_ mechanism that packages up these communications. Normally the client and server would be located on different machines and communicate over a network on a particular numerical "_port_":http://en.wikipedia.org/wiki/Port_(computer_networking), but in this example, they will run on the same machine (and still communicate using ports). In our example, clients and servers will be written in Scala, and the transport will be handled using "Thrift":http://thrift.apache.org/. The primary purpose of this tutorial is to show a simple server and client that can be extended to provide scalable performance. 

h3. Exploring the default bootstrapper project 探索默认的引导程序项目

First, create a skeleton project ("Searchbird") using "scala-bootstrapper":https://github.com/twitter/scala-bootstrapper. This creates a simple "Finagle":http://twitter.github.com/finagle/ -based Scala service that exports an in-memory key-value store. We'll extend this to support searching of the values, and then extend it to support searching multiple in-memory stores over several processes.
首先，使用 "scala-bootstrapper":https://github.com/twitter/scala-bootstrapper 创建一个骨架项目（ “ Searchbird ” ）。这将创建一个简单的基于"Finagle":http://twitter.github.com/finagle/ 并基于key-value内存存储的Scala服务。我们将扩展这个工程支持搜索值，然后扩展到支持多进程多个内存存储的搜索。

<pre>
$ mkdir searchbird ; cd searchbird
$ scala-bootstrapper searchbird
writing build.sbt
writing config/development.scala
writing config/production.scala
writing config/staging.scala
writing config/test.scala
writing console
writing Gemfile
writing project/plugins.sbt
writing README.md
writing sbt
writing src/main/scala/com/twitter/searchbird/SearchbirdConsoleClient.scala
writing src/main/scala/com/twitter/searchbird/SearchbirdServiceImpl.scala
writing src/main/scala/com/twitter/searchbird/config/SearchbirdServiceConfig.scala
writing src/main/scala/com/twitter/searchbird/Main.scala
writing src/main/thrift/searchbird.thrift
writing src/scripts/searchbird.sh
writing src/scripts/config.sh
writing src/scripts/devel.sh
writing src/scripts/server.sh
writing src/scripts/service.sh
writing src/test/scala/com/twitter/searchbird/AbstractSpec.scala
writing src/test/scala/com/twitter/searchbird/SearchbirdServiceSpec.scala
writing TUTORIAL.md
</pre>

Let's first explore the default project @scala-bootstrapper@ creates for us. This is meant as a template. You'll end up substituting most of it, but it serves as a convenient scaffold. It defines a simple (but complete) key-value store. Configuration, a thrift interface, stats export and logging are all included.
首先，来看下 @scala-bootstrapper@为我们创建的默认项目。这是一个模板。你最终将替换它的大部分，不过作为支架它是很方便的。它定义了一个简单的（但完整）key-value存储，并包含配置，thrift接口，统计输出和日志记录。

我们看代码之前，我们要运行一个客户端和服务器，看看它是如何工作的。这里是我们构建的：

Before we look at code, we're going to run a client and server to see how it works. Here's what we're building:

!searchbird-1.svg(Searchbird implementation, revision 1)!

and here is the interface that our service exports. Since the Searchbird service is a "Thrift":http://thrift.apache.org/ service (like most of our services), its external interface is defined in the Thrift IDL ("interface description language").
这里是我们的服务出口的接口。由于的Searchbird服务是一个"Thrift":http://thrift.apache.org/  服务（和我们大部分服务一样），其外部接口使用Thrift IDL（“接口描述语言”）定义。

h5. src/main/thrift/searchbird.thrift

<pre>
service SearchbirdService {
  string get(1: string key) throws(1: SearchbirdException ex)

  void put(1: string key, 2: string value)
}
</pre>

This is pretty straightforward: our service @SearchbirdService@ exports 2 RPC methods, @get@ and @put@. They comprise a simple interface to a key-value store.
这是非常直观的：我们的服务 @SearchbirdService@出输出两个 RPC方法， @get@ 和 @put@。他们组成了一个到key-value存储的简单接口。

现在，让我们运行默认的服务和客户端连接到这个服务，通过这个接口来探索他们。打开两个窗口，一个用于服务器，一个用于客户端。

Now let's run the default service and a client that connects to this service, and explore them through this interface. Open two windows, one for the server and one for the client. 

In the first window, start sbt in interactive mode (run @./sbt@ from the command line[1]) and then build and run the project from within sbt. This runs the @main@ routine in @Main.scala@. 

在第一个窗口，在交互模式启动SBT（在命令行中运行 @./sbt@[1]），然后构建和运行项目内SBT。这会运行@Main.scala@ 定义的 @主@ 进程。

<pre>
$ ./sbt
...
> compile
> run -f config/development.scala
...
[info] Running com.twitter.searchbird.Main -f config/development.scala
</pre>

The configuration file (@development.scala@) instantiates a new service, exposing that service on port 9999 on our local machine. Clients can communicate with this service by connecting to port 9999.
配置文件 (@development.scala@) 实例化一个新的服务，暴露本地机器9999端口上的服务。客户端连接到9999端口使用此服务。

现在，我们将使用 @控制台@ shell脚本初始化和运行一个客户端实例，即 @SearchbirdConsoleClient@实例（@SearchbirdConsoleClient.scala@）。在其他窗口中运行此脚本：
Now we're going to instantiate and run a client using the provided @console@ shell script, which instantiates a @SearchbirdConsoleClient@ instance (from @SearchbirdConsoleClient.scala@). Run this script in the other window:

<pre>
$ ./console 127.0.0.1 9999
[info] Running com.twitter.searchbird.SearchbirdConsoleClient 127.0.0.1 9999
'client' is bound to your thrift client.

finagle-client> 
</pre>

The client object @client@ is now connected to port 9999 on our local computer, and can talk to the service that we previously started on that port. So let's send it some requests:
客户对象@client@现在连接到本地计算机上的9999端口，并可以跟服务交互了。因此，让我们发送一些请求：

<pre>
scala> client.put("marius", "Marius Eriksen")
res0: ...

scala> client.put("stevej", "Steve Jenson")
res1: ...

scala> client.get("marius")
res2: com.twitter.util.Future[String] = ...

scala> client.get("marius").get()
res3: String = Marius Eriksen
</pre>

(The second @get()@ call resolves the @Future@ that is the return type of @client.get()@ by blocking until the value is ready.) 
（第二个 @get()@ 调用解析 @client.get()@ 返回的 @Future@ 类型值，阻塞直到该值准备好了。）

该服务器还输出运行统计（配置文件中指定这些信息在9900端口）。这方便了对各个服务器的检查，以及聚集成全局的服务统计（以机器可读的JSON接口）。打开第三个窗口，并检查这些统计：
The server also exports runtime statistics (the configuration file specifies these can be found at port 9900). These are convenient both for inspecting individual servers as well as aggregating into global service statistics (a machine-readable JSON interface is also provided). Open a third window and check those stats:

<pre>
$ curl localhost:9900/stats.txt
counters:
  Searchbird/connects: 1
  Searchbird/received_bytes: 264
  Searchbird/requests: 3
  Searchbird/sent_bytes: 128
  Searchbird/success: 3
  jvm_gc_ConcurrentMarkSweep_cycles: 1
  jvm_gc_ConcurrentMarkSweep_msec: 15
  jvm_gc_ParNew_cycles: 24
  jvm_gc_ParNew_msec: 191
  jvm_gc_cycles: 25
  jvm_gc_msec: 206
gauges:
  Searchbird/connections: 1
  Searchbird/pending: 0
  jvm_fd_count: 135
  jvm_fd_limit: 10240
  jvm_heap_committed: 85000192
  jvm_heap_max: 530186240
  jvm_heap_used: 54778640
  jvm_nonheap_committed: 89657344
  jvm_nonheap_max: 136314880
  jvm_nonheap_used: 66238144
  jvm_num_cpus: 4
  jvm_post_gc_CMS_Old_Gen_used: 36490088
  jvm_post_gc_CMS_Perm_Gen_used: 54718880
  jvm_post_gc_Par_Eden_Space_used: 0
  jvm_post_gc_Par_Survivor_Space_used: 1315280
  jvm_post_gc_used: 92524248
  jvm_start_time: 1345072684280
  jvm_thread_count: 16
  jvm_thread_daemon_count: 7
  jvm_thread_peak_count: 16
  jvm_uptime: 1671792
labels:
metrics:
  Searchbird/handletime_us: (average=9598, count=4, maximum=19138, minimum=637, p25=637, p50=4265, p75=14175, p90=19138, p95=19138, p99=19138, p999=19138, p9999=19138, sum=38393)
  Searchbird/request_latency_ms: (average=4, count=3, maximum=9, minimum=0, p25=0, p50=5, p75=9, p90=9, p95=9, p99=9, p999=9, p9999=9, sum=14)
</pre>

In addition to our own service statistics, we are also given some generic JVM stats that are often useful.
除了我们自己的服务统计，我们也给出了一些通用的JVM统计。

现在，让我们来看看配置中服务器和客户端的实现代码。

Now let's look at the code that implements the configuration, the server, and the client. 

h5. .../config/SearchbirdServiceConfig.scala

A configuration is a Scala trait that has a method @apply: RuntimeEnvironment => T@ for some @T@ we want to create. In this sense, configurations are "factories". At runtime, a configuration file is evaluated as a script (by using the scala compiler as a library), and is expected to produce such a configuration object. A @RuntimeEnvironment@ is an object queried for various runtime parameters (command-line flags, JVM version, build timestamps, etc.).
配置是一个Scala的特质，有一个方法 @apply: RuntimeEnvironment => T@ 来创建一些 @T@。在这个意义上，配置 是“工厂” 。配置文件在运行时，（通过使用Scala编译器库）被取值为一个脚本，并产生一个配置对象。 @RuntimeEnvironment@ 是一个提供各种运行参数（命令行标志， JVM版本，编译时间戳等）查询的一个对象。

@SearchbirdServiceConfig@ 类指定这样的一个类。它使用它们的默认值一起指定配置参数。 （Finagle 支持一个通用的跟踪系统，我们在本教程将不会介绍；"Zipkin":https://github.com/twitter/zipkin 是一个集合/聚合轨迹的 分布式跟踪系统）。

The @SearchbirdServiceConfig@ class specifies such a class. It specifies configuration parameters together with their defaults. (Finagle supports a generic tracing system that we can ignore for the purposes of this tutorial; the "Zipkin":https://github.com/twitter/zipkin distributed tracing system is a collector/aggregator of such traces.) 

<pre>
class SearchbirdServiceConfig extends ServerConfig[SearchbirdService.ThriftServer] {
  var thriftPort: Int = 9999
  var tracerFactory: Tracer.Factory = NullTracer.factory

  def apply(runtime: RuntimeEnvironment) = new SearchbirdServiceImpl(this)
}
</pre>

In our case, we want to create a @SearchbirdService.ThriftServer@. This is the server type generated by the thrift code generator[2]. 
在我们的例子中，我们要创建一个 @SearchbirdService.ThriftServer@。这是由thrift代码生成器生成的服务器类型[2]。

h5. .../Main.scala

在SBT控制台中键入“run”调用 @main@，这将配置和初始化服务器。它读取配置（在@development.scala@中指定，并会作为参数传给“run”），创建@SearchbirdService.ThriftServer@，并启动它。@RuntimeEnvironment.loadRuntimeConfig@ 执行配置初始化，并把本身作为一个参数来调用@apply@ [3]。
Typing "run" in the sbt console calls @main@, which configures and instantiates this server. It reads the configuration (specified in @development.scala@ and supplied as an argument to "run"), creates the @SearchbirdService.ThriftServer@, and starts it. @RuntimeEnvironment.loadRuntimeConfig@ performs the configuration evaluation and calls its @apply@ method with itself as an argument[3].

<pre>
object Main {
  private val log = Logger.get(getClass)

  def main(args: Array[String]) {
    val runtime = RuntimeEnvironment(this, args)
    val server = runtime.loadRuntimeConfig[SearchbirdService.ThriftServer]
    try {
      log.info("Starting SearchbirdService")
      server.start()
    } catch {
      case e: Exception =>
        log.error(e, "Failed starting SearchbirdService, exiting")
        ServiceTracker.shutdown()
        System.exit(1)
    }
  }
}
</pre>

h5. .../SearchbirdServiceImpl.scala

This is the meat of the service: we extend the @SearchbirdService.ThriftServer@ with our custom implementation. Recall that @SearchbirdService.ThriftServer@ has been created for us by the thrift code generator. It generates a scala method per thrift method. In our example so far, the generated interface is:
这是实质的服务：我们用自己的实现扩展 @SearchbirdService.ThriftServer@。回调 thrift为我们生成的@SearchbirdService.ThriftServer@ 。它为每一个thrift方法生成一个Scala方法。到目前为止，在我们的例子中生成的接口是：

<pre>
trait SearchbirdService {
  def put(key: String, value: String): Future[Void]
  def get(key: String): Future[String]
}
</pre>

@Future[Value]@s are returned instead of the values directly so that their computation may be deferred (finagle's "documentation":finagle.html has more details on @Future@). For the purpose of this tutorial, the only thing you need to know about a @Future@ is that you can retrieve its value with @get()@. 
返回值是@Future[Value]@ 的而不是值可直接可以推迟它们的计算（finagle的"文档":finagle.html 有@Future@更多的细节）。本教程的目的，你唯一需要知道的有关@Future@ 是，可以通过 @get()@检索其值。

@scala-bootstrapper@ 默认实现的key-value存储很简单：它提供了一个通过 @get@ 和 @put@ 访问的 @数据库@ 数据结构。

The default implementation of the key-value store provided by @scala-bootstrapper@ is straightforward: it provides a @database@ data structure and @get@ and @put@ calls that access that data structure. 

<pre>
class SearchbirdServiceImpl(config: SearchbirdServiceConfig) extends SearchbirdService.ThriftServer {
  val serverName = "Searchbird"
  val thriftPort = config.thriftPort
  override val tracerFactory = config.tracerFactory

  val database = new mutable.HashMap[String, String]()

  def get(key: String) = {
    database.get(key) match {
      case None =>
        log.debug("get %s: miss", key)
        Future.exception(SearchbirdException("No such key"))
      case Some(value) =>
        log.debug("get %s: hit", key)
        Future(value)
    }
  }

  def put(key: String, value: String) = {
    log.debug("put %s", key)
    database(key) = value
    Future.Unit
  }

  def shutdown() = {
    super.shutdown(0.seconds)
  }
}
</pre>

The result is a simple thrift interface to a Scala @HashMap@.
其结果是一个对Scala @HashMap@ 的简单thrift接口。

h2. A simple search engine一个简单的搜索引擎

Now we'll extend our example so far to create a simple search engine. We'll then extend it further to become a _distributed_ search engine consisting of multiple shards so that we can fit a corpus larger than what can fit in the memory of a single machine. 
现在，我们将扩展现有的例子，来创建一个简单的搜索引擎。然后，我们将扩展它进一步成为由多个分片自称的 _distributed分布式_ 搜索引擎，使我们能够适应比单台机器内存更大的语料库。

为了简单起见，我们将最小化扩展目前的thrift服务，以支持搜索操作。使用模型是用 @put@ 把文件加入搜索引擎，其中每个文件包含了一系列的记号（词），那么我们就可以输入一串记号，然后搜索返回包含这个串中所有记号的所有文件。该体系结构是与前面的例子相同，但增加了一个新的@search@调用。

To keep things simple, we'll extend our current thrift service minimally in order to support a search operation. The usage model is to @put@ documents into the search engine, where each document contains a series of tokens (words); then we can search on a string of tokens to return all documents that contain all tokens in the set. The architecture is identical to the previous example but for the addition of a new @search@ call.

!searchbird-2.svg(Searchbird implementation, revision 2)!

To implement such a search engine, make the following changes to the following two files:
要实现这样一个搜索引擎需要修改以下两个文件：

h5. src/main/thrift/searchbird.thrift

<pre>
service SearchbirdService {
  string get(1: string key) throws(1: SearchbirdException ex)

  void put(1: string key, 2: string value)

  list<string> search(1: string query)
}
</pre>

We've added a @search@ method that searches the current hashtable, returning the list of keys whose values match the query. The implementation is also straightforward:
我们增加了一个 @search@ 来搜索当前哈希表，返回其值与查询匹配的键列表。实现很简单直观：

h5. .../SearchbirdServiceImpl.scala

Most of our changes take place in this file.
大部分修改都在这个文件中。

现在的@database@ HashMap保存一个正向索引来持有到文档的键映射。我们重命名为@forward@ 并增加一个@倒排reverse@ 索引（映射记号到所有包含该机好的文件）。所以在  @SearchbirdServiceImpl.scala@,中，更换@database@ 定义：

The current @database@ hashmap holds a forward index that maps a key to a document. We rename it to @forward@ and add a second map for the @reverse@ index (that maps a token to the set of documents that contain that token). So, within @SearchbirdServiceImpl.scala@, replace the @database@ definition with:


<pre>
val forward = new mutable.HashMap[String, String]
  with mutable.SynchronizedMap[String, String]
val reverse = new mutable.HashMap[String, Set[String]]
  with mutable.SynchronizedMap[String, Set[String]]
</pre>

Within the @get@ call, replace @database@ with @forward@, but otherwise, @get@ remains the same (it only performs forward lookups). However, @put@ requires changes: we also need to populate the reverse index for each token in the document by appending the document key to the list associated with that token. Replace the @put@ call with the following code. Given a particular search token, we can now use the @reverse@ map to look up documents.

<pre>
def put(key: String, value: String) = {
  log.debug("put %s", key)
  
  forward(key) = value

  // serialize updaters
  synchronized {
    value.split(" ").toSet foreach { token =>
      val current = reverse.getOrElse(token, Set())
      reverse(token) = current + key
    }
  }

  Future.Unit
}
</pre>

Note that (even though the @HashMap@ is thread-safe) only one thread can update the @reverse@ map at a time to ensure that read-modify-write of a particular map entry is an atomic operation. (The code is overly conservative; it locks the entire map rather than locking each individual retrieve-modify-write operation.) Also note the use of @Set@ as the data structure; this ensures that if the same token appears twice in a document, it will only be processed by the @foreach@ loop once. 
需要注意的是（即使 @HashMap@是线程安全的）同时只有一个线程可以更新@倒排@索引，以确保对映射条目的 读-修改-写 是一个原子操作。 （代码是过于保守；它锁定整个映射，而不是锁定每一个条目进行 检索-修改-写 操作。）。另外还要注意使用@Set@作为数据结构；这可以确保即使一个文件中出现两次同样的符号 ，它也只会被@foreach@ 循环处理一次。

这个实现仍然有一个问题，那是留给读者一个练习：当我们用一个新文档覆盖的一个键的时候，我们不删除任何反向索引中引用的旧文件。

The implementation still has an issue that is left as an exercise for the reader: when we overwrite a key with a new document, we don't remove any references to the old document in the reverse index.

Now to the meat of the search engine: the new @search@ method. It should tokenize its query, look up all of the matching documents and, then intersect these lists. This will yield the list of documents that contain all of the tokens in the query. This is straightforward to express in Scala; add this to the @SearchbirdServiceImpl@ class:

现在到搜索引擎的核心：新的@search@ 方法。记号化查询，寻找匹配的文档，然后对这些列表做相交操作。这将产生包含所有查询中的标记文件的列表。在Scala中可以很直接地表达；添加这段代码到@SearchbirdServiceImpl@ 类中：

<pre>
def search(query: String) = Future.value {
  val tokens = query.split(" ")
  val hits = tokens map { token => reverse.getOrElse(token, Set()) }
  val intersected = hits reduceLeftOption { _ & _ } getOrElse Set()
  intersected.toList
}
</pre>

A few things are worth calling out in this short piece of code. When constructing the hit list, if the key (@token@) is not found, @getOrElse@ will supply the value from its second parameter (in this case, an empty @Set@). We perform the actual intersection using a left-reduce. The particular flavor, @reduceLeftOption@, will not attempt to perform the reduce if @hits@ is empty, returning instead @None@. This allows us to supply a default value instead of experiencing an exception. In fact, this is equivalent to:
在这一段短短的代码中有几件事情是值得关注的。在构建命中列表时，如果键（@token@）没有被发现，@getOrElse@ 会返回其第二个参数（在这种情况下，一个空@Set集@）。我们使用left-reduce执行实际的交集。特定的@reduceLeftOption@发现@hits@为空时将不会尝试执行继续执行reduce操作。这使我们能够提供一个默认值，而不是抛出一个异常。其实，这相当于：

<pre>
def search(query: String) = Future.value {
  val tokens = query.split(" ")
  val hits = tokens map { token => reverse.getOrElse(token, Set()) }
  if (hits.isEmpty)
    Nil
  else
    hits reduceLeft { _ & _ } toList
}
</pre>

Which to use is mostly a matter of taste, though functional style often eschews conditionals for sensible defaults.
使用那种方式大多是个人喜好的问题，虽然函数式风格往往会避开带有合理默认值的条件语句。

现在，我们可以尝试在控制台使用我们的新的实现。重启服务器：
We can now experiment with our new implementation using the console. Start your server again:

<pre>
$ ./sbt
...
> compile
> run -f config/development.scala
...
[info] Running com.twitter.searchbird.Main -f config/development.scala
</pre>

and then from the searchbird directory, start up a client:
然后再从searchbird目录，启动客户端：

<pre>
$ ./console 127.0.0.1 9999
...
[info] Running com.twitter.searchbird.SearchbirdConsoleClient 127.0.0.1 9999
'client' is bound to your thrift client.

finagle-client> 
</pre>

Paste the following lecture descriptions into the console:
粘贴以下说明到控制台：

<!--
grep -h '^(desc|title):' ../web/_posts/* | tr A-Z a-z | tr '=''\-+.,:' ' ' | awk '
/^title/ { title=$2 }
/^desc/ {
	d=""
	for(i = 2; i < NF; i++) { d = d " " $i }
	print "$client.put(\"" title "\", \"" d "\")"
}'
-->

<pre>
client.put("basics", " values functions classes methods inheritance try catch finally expression oriented")
client.put("basics", " case classes objects packages apply update functions are objects (uniform access principle) pattern")
client.put("collections", " lists maps functional combinators (map foreach filter zip")
client.put("pattern", " more functions! partialfunctions more pattern")
client.put("type", " basic types and type polymorphism type inference variance bounds")
client.put("advanced", " advanced types view bounds higher kinded types recursive types structural")
client.put("simple", " all about sbt the standard scala build")
client.put("more", " tour of the scala collections")
client.put("testing", " write tests with specs a bdd testing framework for")
client.put("concurrency", " runnable callable threads futures twitter")
client.put("java", " java interop using scala from")
client.put("searchbird", " building a distributed search engine using")
</pre>

We can now perform some searches, which return the keys of the documents that contain the search terms. 
现在，我们可以执行一些搜索，返回键其中包含搜索词的文件。

<pre>
> client.search("functions").get()
res12: Seq[String] = ArrayBuffer(basics)

> client.search("java").get()
res13: Seq[String] = ArrayBuffer(java)

> client.search("java scala").get()
res14: Seq[String] = ArrayBuffer(java)

> client.search("functional").get()
res15: Seq[String] = ArrayBuffer(collections)

> client.search("sbt").get()
res16: Seq[String] = ArrayBuffer(simple)

> client.search("types").get()
res17: Seq[String] = ArrayBuffer(type, advanced)
</pre>

Recall that if the call returns a @Future@, we have to use a blocking @get()@ call to resolve the value contained within that future. We can use the @Future.collect@ command to make multiple concurrent requests and wait for all of them to succeed:
回想一下，如果调用返回一个@Future@，我们必须使用一个阻塞的 @get()@ 来获取其中包含的值。我们可以使用 @Future.collect@ 命令来创建多个并发请求，并等待所有这些成功返回：

<pre>
> import com.twitter.util.Future
...
> Future.collect(Seq(
    client.search("types"),
    client.search("sbt"),
    client.search("functional")
  )).get()
res18: Seq[Seq[String]] = ArrayBuffer(ArrayBuffer(type, advanced), ArrayBuffer(simple), ArrayBuffer(collections))
</pre>

h2. Distributing our service
分发我们的服务

一台机器上，一个简单的内存搜索引擎将无法搜索超过内存大小的语料库。现在，我们要大胆改进，用一个简单的分片计划来构建分布式节点。下面的框图：
On a single machine, our simple in-memory search engine won't be able to search a corpus larger than the size of memory. We'll now venture to remedy this by distributing nodes with a simple sharding scheme. Here's the block diagram:

!searchbird-3.svg(Distributed Searchbird service)!

h3. Abstracting抽象

To aid our work, we'll first introduce another abstraction--an @Index@--in order to decouple the index implementation from the @SearchbirdService@. This is a straightforward refactor. We'll begin by adding an Index file to the build (create the file @searchbird/src/main/scala/com/twitter/searchbird/Index.scala@):
为了帮助我们的工作，我们会先介绍另一个抽象--@索引@--来解耦@SearchbirdService@对索引实现的依赖。这是一个简单的重构。我们首先添加一个索引文件到构建 (创建文件 @searchbird/src/main/scala/com/twitter/searchbird/Index.scala@):

h5. .../Index.scala

<pre>
package com.twitter.searchbird

import scala.collection.mutable
import com.twitter.util._
import com.twitter.conversions.time._
import com.twitter.logging.Logger
import com.twitter.finagle.builder.ClientBuilder
import com.twitter.finagle.thrift.ThriftClientFramedCodec

trait Index {
  def get(key: String): Future[String]
  def put(key: String, value: String): Future[Unit]
  def search(key: String): Future[List[String]]
}

class ResidentIndex extends Index {
  val log = Logger.get(getClass)

  val forward = new mutable.HashMap[String, String]
    with mutable.SynchronizedMap[String, String]
  val reverse = new mutable.HashMap[String, Set[String]]
    with mutable.SynchronizedMap[String, Set[String]]

  def get(key: String) = {
    forward.get(key) match {
      case None =>
        log.debug("get %s: miss", key)
        Future.exception(SearchbirdException("No such key"))
      case Some(value) =>
        log.debug("get %s: hit", key)
        Future(value)
    }
  }

  def put(key: String, value: String) = {
    log.debug("put %s", key)
    
    forward(key) = value

    // admit only one updater.
    synchronized {
      (Set() ++ value.split(" ")) foreach { token =>
        val current = reverse.get(token) getOrElse Set()
        reverse(token) = current + key
      }
    }

    Future.Unit
  }

  def search(query: String) = Future.value {
    val tokens = query.split(" ")
    val hits = tokens map { token => reverse.getOrElse(token, Set()) }
    val intersected = hits reduceLeftOption { _ & _ } getOrElse Set()
    intersected.toList
  }
}
</pre>

We now convert our thrift service to a simple dispatch mechanism: it provides a thrift interface to any @Index@ instance. This is a powerful abstraction, because it separates the implementation of the service from the implementation of the index. The service no longer has to know any details of the underlying index; the index might be local or might be remote or might be a composite of many remote indices, but the service doesn't care, and the implementation of the index might change without the service changing. 
现在，我们把thrift服务转换成一个简单的调度机制：为每一个@索引@实例提供了一个thrift接口。这是一个强大的抽象，因为它把索引实现和服务实现分离开了。该服务不再知道索引的任何细节；索引可以是本地或远程，甚至可能是一个许多索引的组合，但服务并不关心，索引实现可能会更改的但是不用改变服务。

将@SearchbirdServiceImpl@ 类定义更换为以下（简单得多）的代码（其中不再包含索引实现细节）。注意初始化服务器现在需要第二个参数 @Index@。
Replace your @SearchbirdServiceImpl@ class definition with the (much simpler) one below (which no longer contains the index implementation detail). Note initializing a server now takes a second argument, an @Index@.

h5. .../SearchbirdServiceImpl.scala

<pre>
class SearchbirdServiceImpl(config: SearchbirdServiceConfig, index: Index) extends SearchbirdService.ThriftServer {
  val serverName = "Searchbird"
  val thriftPort = config.thriftPort

  def get(key: String) = index.get(key)
  def put(key: String, value: String) =
    index.put(key, value) flatMap { _ => Future.Unit }
  def search(query: String) = index.search(query)

  def shutdown() = {
    super.shutdown(0.seconds)
  }
}
</pre>

h5. .../config/SearchbirdServiceConfig.scala

Update your @apply@ call in @SearchbirdServiceConfig@ accordingly:
相应更新 @SearchbirdServiceConfig@ 的@apply@调用：

<pre>
class SearchbirdServiceConfig extends ServerConfig[SearchbirdService.ThriftServer] {
  var thriftPort: Int = 9999
  var tracerFactory: Tracer.Factory = NullTracer.factory

  def apply(runtime: RuntimeEnvironment) = new SearchbirdServiceImpl(this, new ResidentIndex)
}
</pre>

We'll set up our simple distributed system so that there is one distinguished node that coordinates queries to its child nodes. In order to achieve this, we'll need two new @Index@ types. One represents a remote index, the other is a composite index over several other @Index@ instances. This way we can construct the distributed index by instantiating a composite index of the remote indices. Note that both @Index@ types have the same interface, so servers do not need to know whether the index to which they are connected is remote or composite. 
我们将建立我们简单的分布式系统，一个主节点组织查询其子节点。为了实现这一目标，我们将需要两个新的 @Index@类型。一个代表远程索引，另一种是其他多个@Index@ 实例的组合索引。这样我们的服务就可以实例化多个远程索引的复合索引来构建分布式索引。请注意这两个 @Index@类型具有相同的接口，所以服务器不需要知道它们所连接的索引是远程的还是复合的。

h5. .../Index.scala

In @Index.scala@, define a @CompositeIndex@:
在@Index.scala@中定义了@CompositeIndex@:

<pre>
class CompositeIndex(indices: Seq[Index]) extends Index {
  require(!indices.isEmpty)

  def get(key: String) = {
    val queries = indices.map { idx =>
      idx.get(key) map { r => Some(r) } handle { case e => None }
    }

    Future.collect(queries) flatMap { results =>
      results.find { _.isDefined } map { _.get } match {
        case Some(v) => Future.value(v)
        case None => Future.exception(SearchbirdException("No such key"))
      }
    }
  }

  def put(key: String, value: String) =
    Future.exception(SearchbirdException("put() not supported by CompositeIndex"))

  def search(query: String) = {
    val queries = indices.map { _.search(query) rescue { case _=> Future.value(Nil) } }
    Future.collect(queries) map { results => (Set() ++ results.flatten) toList }
  }
}
</pre>

The composite index works over a set of underlying @Index@ instances. Note that it doesn't care how these are actually implemented. This type of composition allows for great flexibility in constructing various querying schemes. We don't define a sharding scheme, and so the composite index doesn't support @put@ operations. These are instead issued directly to the child nodes. @get@ is implemented by querying all of our child nodes and picking the first successful result. If there are none, we throw an exception. Note that since the absence of a value is communicated by throwing an exception, we @handle@ this on the @Future@, converting any exception into a @None@ value. In a real system, we'd probably have proper error codes for missing values rather than using exceptions. Exceptions are convenient and expedient for prototyping, but compose poorly. In order to distinguish between a real exception and a missing value, I have to examine the exception itself. Rather, it is better style to embed this distinction directly in the type of the returned value.
组合指数构建在一组相关@Index@实例上。请注意，它并不关心这些实际上是如何实现的。这种类型的组合在构建不同的查询机制的时候有极大的灵活性。我们不定义拆分机制，所以复合索引不支持@put@操作。这些直接由子节点处理TODO。 @get@的实现是查询我们的所有子节点，并提取第一个成功的结果。如果没有的话，我们抛出一个异常。注意通过抛出一个异常，因为没有值传递，我们使用@handle@ 来处理@Future@ ，将任何异常转换成 @None@。在实际系统中，我们很可能为遗漏填入适当的错误码，而不是使用异常。异常在构建原型时是方便和适宜的，但组合的不好。为了区分一个真正的例外和遗漏值，我要检查异常本身。相反，把这种区别直接嵌入在返回值的类型中是更好的风格。

<!-- *_HELP This implementation appears to not give any more scalability than the previous scheme; since the index appears to be completely replicated across all client machines, we can't store a larger amount of data. We'd require a more sophisticated @put()@ scheme that distributed puts to only one index, wouldn't we? Alternately, we could improve throughput by only sending @get()@ requests to one node rather than all nodes._* -->

@search@ works in a similar way as before. Instead of picking the first result, we combine them, ensuring their uniqueness by using a @Set@ construction.
@search@ 像以前一样工作。和提取第一个结果不同，我们把它们组合起来，通过使用@@Set@@确保其单一性。

@RemoteIndex@ 提供了到远程服务器的一个@Index@接口。
@RemoteIndex@ provides an @Index@ interface to a remote server. 

<pre>
class RemoteIndex(hosts: String) extends Index {
  val transport = ClientBuilder()
    .name("remoteIndex")
    .hosts(hosts)
    .codec(ThriftClientFramedCodec())
    .hostConnectionLimit(1)
    .timeout(500.milliseconds)
    .build()
  val client = new SearchbirdService.FinagledClient(transport)

  def get(key: String) = client.get(key)
  def put(key: String, value: String) = client.put(key, value) map { _ => () }
  def search(query: String) = client.search(query) map { _.toList }
}
</pre>

This constructs a finagle thrift client with some sensible defaults, and just proxies the calls, adjusting the types slightly.
这样使用一些合理的默认值构造一个finagle thrift客户端，而且只是代理调用，稍微调整类型。

h3. Putting it all together全部放在一起

We now have all the pieces we need. We'll need to adjust the configuration in order to be able to invoke a given node as either a distinguished node or a data shard node. In order to do so, we'll enumerate the shards in our system by creating a new config item for it. We also need to add the @Index@ argument to our instantiation of the @SearchbirdServiceImpl@. We'll then use command line arguments (recall that the @Config@ has access to these) to start the server up in either mode.

现在，我们有所需要的所有功能。我们需要调整配置，以便能够调用一个给定的节点，主节点亦或是数据分片节点。为了做到这一点，我们将通过创建一个新的配置项来在系统中枚举分片。我们还需要添加 @Index@ 参数到我们的 @SearchbirdServiceImpl@实例。然后，我们将使用命令行参数（还记得 @Config@对这些的访问TODO）在这两种模式中启动服务器。

h5. .../config/SearchbirdServiceConfig.scala

<pre>
class SearchbirdServiceConfig extends ServerConfig[SearchbirdService.ThriftServer] {
  var thriftPort: Int = 9999
  var shards: Seq[String] = Seq()

  def apply(runtime: RuntimeEnvironment) = {
    val index = runtime.arguments.get("shard") match {
      case Some(arg) =>
        val which = arg.toInt
        if (which >= shards.size || which < 0)
          throw new Exception("invalid shard number %d".format(which))

        // override with the shard port
        val Array(_, port) = shards(which).split(":")
        thriftPort = port.toInt

        new ResidentIndex

      case None =>
        require(!shards.isEmpty)
        val remotes = shards map { new RemoteIndex(_) }
        new CompositeIndex(remotes)
    }

    new SearchbirdServiceImpl(this, index)
  }
}
</pre>

Now we'll adjust the configuration itself: add the "shards" initialization to the instantiation of @SearchbirdServiceConfig@ (we can talk to shard 0 via port 9000, shard 1 via port 9001, and so on). 
现在，我们将调整配置：添加了“分片”初始化到 @SearchbirdServiceConfig@的初始化中（我们可以通过端口9000访问分片0，9001访问分片1，依次类推）。

h5. config/development.scala

<pre>
new SearchbirdServiceConfig {
  // Add your own config here
  shards = Seq(
    "localhost:9000",
    "localhost:9001",
    "localhost:9002"
  )
  ...
</pre>

Comment out the setting for @admin.httpPort@ (we don't want several services running on the same machine, all of which are trying to open up the same port):
注释掉@admin.httpPort@的设置（我们不希望在同一台机器上运行多个服务，而这写服务都试图打开相同的端口）：

<pre>
  // admin.httpPort = 9900
</pre>

Now if we invoke our server without any arguments, it starts a distinguished node that speaks to all of the given shards. If we specify a shard argument, it starts a server on the port belonging to that shard index.
现在，如果我们不带任何参数调用我们的服务器，它启动一个主节点来和所有分片通信。如果我们指定一个分片参数，它会在指定端口启动一个分片服务器。

让我们试试吧！我们将启动3个服务：2个分片和1个主节点。首先编译改动：
Let's try it! We'll launch 3 services: 2 shards and 1 distinguished node. First compile the changes:

<pre>
$ ./sbt
> compile
...
> exit
</pre>

Then launch 3 servers:
然后启动三个服务：

<pre>
$ ./sbt 'run -f config/development.scala -D shard=0'
$ ./sbt 'run -f config/development.scala -D shard=1'
$ ./sbt 'run -f config/development.scala'
</pre>

You can either run these in 3 different windows or (in the same window) start each one in turn, wait for it to start up, control-z to suspend it, and @bg@ to put it running in the background. 
您可以运行在3个不同的窗口或在同一窗口开始依次逐个，等待开始，控制-Z暂停，和@ BG@把它在后台运行。

然后，我们将与他们进行互动，通过控制台。首先，让我们填充一些数据在两个碎片节点。运行从searchbird目录：
Then we'll interact with them through the console. First, let's populate some data in the two shard nodes. Running from the searchbird directory:

<pre>
$ ./console localhost 9000
...
> client.put("fromShardA", "a value from SHARD_A")
> client.put("hello", "world")
</pre>
<pre>
$ ./console localhost 9001
...
> client.put("fromShardB", "a value from SHARD_B")
> client.put("hello", "world again")
</pre>

You can exit these console sessions once you complete them.  Now query our database from the distinguished node (port 9999): 
一旦你完成，你可以退出这些控制台会话。现在，通过主节点查询我们的数据库（9999端口）：

<pre>
$ ./console localhost 9999
[info] Running com.twitter.searchbird.SearchbirdConsoleClient localhost 9999
'client' is bound to your thrift client.

finagle-client> client.get("hello").get()
res0: String = world

finagle-client> client.get("fromShardC").get()
SearchbirdException(No such key)
...

finagle-client> client.get("fromShardA").get()
res2: String = a value from SHARD_A

finagle-client> client.search("hello").get()
res3: Seq[String] = ArrayBuffer()

finagle-client> client.search("world").get()
res4: Seq[String] = ArrayBuffer(hello)

finagle-client> client.search("value").get()
res5: Seq[String] = ArrayBuffer(fromShardA, fromShardB)
</pre>

This design has multiple data abstractions that allow a more modular and scalable implementation:
* The @ResidentIndex@ data structure knows nothing about the network, servers, or clients.
* The @CompositeIndex@ knows nothing about how its constituent indices are implemented or their underlying data structures; it simply distributes its requests to them. 
* The same @search@ interface (trait) for servers allows a server to query its local data structure (@ResidentIndex@) or distribute queries to other servers (@CompositeIndex@) without needing to know this distinction, which is hidden from the caller.
* The @SearchbirdServiceImpl@ and @Index@ are separate modules now, allowing a simple implementation of the service and separating the implementation of the data structure from the service that accesses it. 
* The design is flexible enough to allow one or many remote indices, located on the local machine or on remote machines. 

这种设计有多个数据抽象，允许更加模块化和可扩展的实现：
* @ResidentIndex@ 数据结构对网络、服务器或客户端一无所知。
* @CompositeIndex@ 对其构成索引的底层数据结构和组合方式一无所知；它只是把请求分配给他们。
* 服务器相同的@search@接口（特质）允许服务器查询其本地数据结构(@ResidentIndex@) ，或分发到其他服务器(@CompositeIndex@) 查询，而不需要知道这个区别，这是从调用隐藏的。
* @SearchbirdServiceImpl@和@Index@ 现在是相互独立的模块，让一个简单的实现服务和服务访问的数据结构的实现分离了。
* 灵活的设计允许一个或多个远程索引在本地机器或远程机器上。

<!-- *_HELP Are the possible improvements below accurate?_* -->

Possible improvements to this implementation would include:
这个实施的可能改进将包括：

* The current implementation sends @put()@ calls to all nodes. Instead, we could use a hash table to send a @put()@ call to only one node and distribute storage across all nodes.
** Note, however, we lose redundancy with this strategy. How could we maintain some redundancy yet not require full replication? 
* We aren't doing anything interesting with any failures in the system (we aren't processing any exceptions, for instance). 
*当前的实现@put()@调用发送到所有节点。相反，我们可以使用一个哈希表发送一个@put()@调用只到一个节点，而在所有节点之间分配存储。
** 注，但是，我们失去了这个策略的冗余。我们怎么能保持一定的冗余度，但不需要完全复制？
* 当系统出错时我们没有做任何有趣的处理（例如我们不处理任何异常） 。

fn1. The local @./sbt@ script simply guarantees that the sbt version is consistent with one that we know has all the proper libraries available.

fn2. In @target/gen-scala/com/twitter/searchbird/SearchbirdService.scala@.

fn3. See Ostrich's "README":https://github.com/twitter/ostrich/blob/master/README.md for more information.

fn1. 本地 @./sbt@脚本只是保证该SBT版本和我们知道所有正确的库是一致的。

fn2. 在@target/gen-scala/com/twitter/searchbird/SearchbirdService.scala@ 。

fn3. 更多信息见Ostrich's "README":https://github.com/twitter/ostrich/blob/master/README.md。